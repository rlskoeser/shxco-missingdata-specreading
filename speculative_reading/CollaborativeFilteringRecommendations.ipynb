{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S&Co Collaborative Filtering Notebook\n",
    "\n",
    "This notebook implements collaborative filtering methods for S&Co data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import altair as alt\n",
    "alt.data_transformers.disable_max_rows()\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from speculative_reading.load_datasets import get_updated_shxco_data\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "from scipy import spatial, stats\n",
    "from scipy.stats import zscore, mode\n",
    "# from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "members_df, books_df, borrow_events, events_df = get_updated_shxco_data(get_subscription=False)\n",
    "partial_df = pd.read_csv('../data/partial_borrowers_collapsed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_author_title(item: pd.DataFrame) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Get author and title from a book item\n",
    "\n",
    "    Args:\n",
    "    item: pd.DataFrame: a DataFrame containing a book item\n",
    "\n",
    "    Returns:\n",
    "    Tuple[str, str]: a tuple containing the author and title of the book\n",
    "    \"\"\"\n",
    "    if item.author.isna().any() == False:\n",
    "        author = ' '.join(item.author.str.split(',').values[0][::-1])\n",
    "    else: \n",
    "        author = '(Periodical)'\n",
    "    title = item.title.values[0]\n",
    "    return author, title\n",
    "\n",
    "def get_formatted_titles(sorted_values: pd.DataFrame, numb_of_preds: int, is_table: bool = False) -> List[str]:\n",
    "    \"\"\"\n",
    "    Get a list of formatted titles\n",
    "\n",
    "    Args:\n",
    "    sorted_values: pd.DataFrame: a DataFrame containing the sorted values\n",
    "    numb_of_preds: int: the number of predictions to return\n",
    "    is_table: bool: whether to return the titles in table format\n",
    "\n",
    "    Returns:\n",
    "    List[str]: a list of formatted titles\n",
    "    \"\"\"\n",
    "    titles = []\n",
    "    for i in sorted_values[0:numb_of_preds].item_uri.tolist():\n",
    "        item = books_df[books_df.id == i]\n",
    "        author, title = get_author_title(item)\n",
    "        if is_table:\n",
    "            titles.append(f\"*{title}*,<br>{author}\")\n",
    "        else:\n",
    "            titles.append(f\"{title} by {author}\")\n",
    "    return titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euc_distance(prefs: Dict[str, Dict[str, float]], person1: str, person2: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the Euclidean distance between two people based on their preferences.\n",
    "    \"\"\"\n",
    "    # Find common items\n",
    "    si = {item: 1 for item in prefs[person1] if item in prefs[person2]}\n",
    "    \n",
    "    # If they have no common items, return 0\n",
    "    if len(si) == 0: \n",
    "        return 0\n",
    "\n",
    "    # Calculate the sum of squares of differences\n",
    "    sum_of_squares = sum(pow(prefs[person1][item] - prefs[person2][item], 2) for item in si)\n",
    "    \n",
    "    # Return the inverse of Euclidean distance to give a higher value to people who are more similar\n",
    "    return 1 / (1 + sum_of_squares)\n",
    "\n",
    "def calculate_rankings(prefs: Dict[str, Dict[str, float]], person: str, other: str, score: float, totals: Dict[str, float], simSums: Dict[str, float], query_books: List[str]) -> List[Tuple[float, str]]:\n",
    "    \"\"\"\n",
    "    Calculate the rankings of books for a person based on the preferences of another person and a similarity score.\n",
    "    \"\"\"\n",
    "    for item in prefs[other]:\n",
    "        # Only consider books that the person hasn't read or rated yet and are in the query books\n",
    "        if item not in prefs[person] or prefs[person][item] == 0:\n",
    "            if item in query_books:\n",
    "                # Calculate the total score for each book\n",
    "                totals.setdefault(item, 0) \n",
    "                totals[item] += prefs[other][item] * score\n",
    "                # Calculate the total similarity for each book\n",
    "                simSums.setdefault(item, 0)\n",
    "                simSums[item] += score\n",
    "\n",
    "    # Calculate the average score for each book\n",
    "    rankings = [(total / simSums[item], item) for item, total in totals.items()]\n",
    "    # Sort the rankings in descending order\n",
    "    rankings.sort()\n",
    "    rankings.reverse()\n",
    "    return rankings\n",
    "\n",
    "def get_predictions(person: str, prefs: Dict[str, Dict[str, float]], weighted: pd.DataFrame,  query_books: List[str]) -> Tuple[List[Tuple[float, str]], List[Tuple[float, str]], List[Tuple[float, str]]]:\n",
    "    \"\"\"\n",
    "    Get book recommendations for a person based on the preferences of all other people.\n",
    "    \"\"\"\n",
    "    totals = {} \n",
    "    simSums = {}\n",
    "\n",
    "    cos_totals = {} \n",
    "    cos_simSums = {}\n",
    "\n",
    "    pear_totals = {} \n",
    "    pear_simSums = {}\n",
    "\n",
    "    for other in prefs:\n",
    "        # Don't compare the person to themselves\n",
    "        if other == person:\n",
    "            continue\n",
    "\n",
    "        # Get the preference vectors for the person and the other person\n",
    "        member_vector = weighted.loc[person]\n",
    "        other_vector = weighted.loc[other]\n",
    "\n",
    "        # Calculate the similarity scores\n",
    "        euc_score = euc_distance(prefs, person, other)\n",
    "        cosine_score = spatial.distance.cosine(member_vector, other_vector)\n",
    "        pearson_score, pearson_p = stats.pearsonr(member_vector, other_vector)\n",
    "\n",
    "        # Ignore people who have a similarity score of 0 with the person\n",
    "        if euc_score <= 0:\n",
    "            continue\n",
    "\n",
    "        # Calculate the rankings based on each similarity score\n",
    "        rankings = calculate_rankings(prefs, person, other, euc_score, totals, simSums, query_books)\n",
    "        cos_rankings = calculate_rankings(prefs, person, other, cosine_score, cos_totals, cos_simSums, query_books)\n",
    "        pear_rankings = calculate_rankings(prefs, person, other, pearson_score, pear_totals, pear_simSums, query_books)\n",
    "\n",
    "    return rankings, cos_rankings, pear_rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rankings_dataframe(rankings: List[Tuple[float, str]], score_type: str, n: int, return_all: bool) -> pd.DataFrame:\n",
    "\t\"\"\"\n",
    "\tCreate a DataFrame from the rankings of books.\n",
    "\n",
    "\tParameters:\n",
    "\trankings: A list of tuples where each tuple contains a score and an item URI.\n",
    "\tscore_type: A string indicating the type of score used in the rankings.\n",
    "\tn: The number of top items to include in the DataFrame.\n",
    "\treturn_all: A boolean indicating whether to return all items in the rankings.\n",
    "\n",
    "\tReturns:\n",
    "\tA DataFrame with the top n items from the rankings.\n",
    "\t\"\"\"\n",
    "\t# Create a DataFrame from the top n items in the rankings.\n",
    "\tif return_all:\n",
    "\t\tn = len(rankings)\n",
    "\tdf = pd.DataFrame(rankings[0:n])\n",
    "\t# Set the column names.\n",
    "\tdf.columns = ['score', 'item_uri']\n",
    "\n",
    "\t# Get the formatted titles for the chart and add them to the DataFrame.\n",
    "\tchart_titles = get_formatted_titles(df, n, True)\n",
    "\tdf['formatted_chart_title'] = chart_titles\n",
    "\n",
    "\t# Get the formatted titles for the table and add them to the DataFrame.\n",
    "\ttable_titles = get_formatted_titles(df, n, False)\n",
    "\tdf['formatted_table_title'] = table_titles\n",
    "\n",
    "\t# Add the score type to the DataFrame.\n",
    "\tdf['type'] = score_type\n",
    "\n",
    "\treturn df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_col(df: pd.DataFrame, cols: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Scale specified columns in a DataFrame using MinMaxScaler.\n",
    "\n",
    "    Parameters:\n",
    "    df: A pandas DataFrame.\n",
    "    cols: A list of column names to scale.\n",
    "\n",
    "    Returns:\n",
    "    A DataFrame with the scaled columns.\n",
    "    \"\"\"\n",
    "    # For each column in the list of columns...\n",
    "    for col in cols:\n",
    "        # Create a MinMaxScaler.\n",
    "        scaler = MinMaxScaler()\n",
    "        # Fit the scaler to the column and transform the column.\n",
    "        # The 'reshape' function is used to reshape the column to a 2D array, which is required by the 'fit_transform' function.\n",
    "        # The transformed column is added to the DataFrame with the suffix '_scaled'.\n",
    "        df[col + '_scaled'] = scaler.fit_transform(df[col].values.reshape(-1, 1))\n",
    "    # Return the DataFrame with the scaled columns.\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_rankings(borrow_events: pd.DataFrame, partial_df: pd.DataFrame, keep_periodicals: bool) -> Tuple[Dict[str, Dict[str, int]], pd.DataFrame]:\n",
    "\t\"\"\"\n",
    "\tProcess the rankings of books and create a DataFrame with the top n items.\n",
    "\n",
    "\tParameters:\n",
    "\tborrow_events: A DataFrame containing the borrow events data.\n",
    "\tpartial_df: A DataFrame containing the partial borrowers data.\n",
    "\tkeep_periodicals: A boolean indicating whether to keep periodicals in the rankings.\n",
    "\n",
    "\tReturns:\n",
    "\tA dictionary where the keys are member IDs and the values are dictionaries of item URIs and counts.\n",
    "\tA DataFrame with the top n items from the rankings.\n",
    "\t\"\"\"\n",
    "\t# Group the borrow events by member ID and item URI, and count the number of events for each combination.\n",
    "\t# This can help us understand how many times each member borrowed each item.\n",
    "\tif keep_periodicals:\n",
    "\t\tgrouped_borrows = borrow_events[['member_id', 'item_uri']].groupby(['member_id', 'item_uri']).size().reset_index(name='counts')\n",
    "\telse:\n",
    "\t\tgrouped_borrows = borrow_events[borrow_events.item_uri.isin(books_df[books_df.author.notna()].uri)][['member_id', 'item_uri']].groupby(['member_id', 'item_uri']).size().reset_index(name='counts')\n",
    "\n",
    "\t# Pivot the grouped borrow events to create a matrix where the rows are member IDs, the columns are item URIs, and the values are counts.\n",
    "\t# Fill missing values with 0 and convert the counts to integers.\n",
    "\t# This creates a user-item matrix that can be used for collaborative filtering.\n",
    "\tpivoted_borrows = grouped_borrows.pivot(\n",
    "\t\tindex='member_id', columns='item_uri', values='counts').fillna(0).astype(int)\n",
    "\n",
    "\t# Standardize the user-item matrix by subtracting the mean and dividing by the standard deviation.\n",
    "\t# This can help improve the performance of the collaborative filtering algorithm.\n",
    "\tscaler = StandardScaler()\n",
    "\tweighted = scaler.fit_transform(pivoted_borrows)\n",
    "\tweighted = pd.DataFrame(weighted, index=pivoted_borrows.index, columns=pivoted_borrows.columns)\n",
    "\n",
    "\t# Define a list of members to query.\n",
    "\tpartial_members = ['hemingway-ernest']\n",
    "\n",
    "\t# Get the unique member IDs from the grouped borrow events that are in the query members list.\n",
    "\tquery_members = grouped_borrows.loc[grouped_borrows.member_id.isin(partial_members)].member_id.unique().tolist()\n",
    "\n",
    "\t# Filter the partial_df DataFrame to include only rows where the member ID is in the query members list.\n",
    "\tpartial_df = partial_df[partial_df.member_id.isin(query_members)]\n",
    "\n",
    "\t# Convert the subscription start and end times to datetime objects.\n",
    "\tpartial_df['subscription_starttime'] = pd.to_datetime(partial_df['subscription_start'])\n",
    "\tpartial_df['subscription_endtime'] = pd.to_datetime(partial_df['subscription_end'])\n",
    "\n",
    "\t# Create a dictionary where the keys are member IDs and the values are dictionaries of item URIs and counts.\n",
    "\t# This can be used to quickly look up the count for a specific member and item.\n",
    "\tmember_item_counts = (grouped_borrows.groupby(['member_id'])['item_uri', 'counts'].apply(\n",
    "\t\tlambda x: dict(x.values)).to_dict())\n",
    "\treturn member_item_counts, weighted, partial_df\n",
    "\n",
    "member_item_counts, weighted, partial_df = process_rankings(borrow_events, partial_df, keep_periodicals=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating predictions for all members, but limited to circulation periods...\n",
      "Processing index: 0\n",
      "Subscription period: 1921-12-28 to 1922-11-08\n",
      "Identified events: 2602\n",
      "Identified books: 666\n",
      "Identified books without those read by member: 660\n",
      "Euclidean: 469, Cosine: 469, Pearson: 469\n",
      "Processing index: 1\n",
      "Subscription period: 1924-03-28 to 1925-03-28\n",
      "Identified events: 6112\n",
      "Identified books: 1292\n",
      "Identified books without those read by member: 1280\n",
      "Identified books without those read by member and those read in previous subscription: 620\n",
      "Euclidean: 477, Cosine: 477, Pearson: 477\n",
      "Calculating predictions for all members...\n",
      "Processing index: 0\n",
      "Subscription period: 1921-12-28 to 1922-11-08\n",
      "Identified events: 2602\n",
      "Identified books: 666\n",
      "Identified books without those read by member: 660\n",
      "Euclidean: 469, Cosine: 469, Pearson: 469\n",
      "Processing index: 1\n",
      "Subscription period: 1924-03-28 to 1925-03-28\n",
      "Identified events: 6112\n",
      "Identified books: 1292\n",
      "Identified books without those read by member: 1280\n",
      "Euclidean: 946, Cosine: 946, Pearson: 946\n",
      "Calculating predictions for top 200 members, but limited to circulation periods...\n",
      "Processing index: 0\n",
      "Subscription period: 1921-12-28 to 1922-11-08\n",
      "Identified events: 2602\n",
      "Identified books: 666\n",
      "Identified books without those read by member: 660\n",
      "Euclidean: 469, Cosine: 469, Pearson: 469\n",
      "Processing index: 1\n",
      "Subscription period: 1924-03-28 to 1925-03-28\n",
      "Identified events: 6112\n",
      "Identified books: 1292\n",
      "Identified books without those read by member: 1280\n",
      "Identified books without those read by member and those read in previous subscription: 620\n",
      "Euclidean: 477, Cosine: 477, Pearson: 477\n",
      "Calculating predictions for top 200 members...\n",
      "Processing index: 0\n",
      "Subscription period: 1921-12-28 to 1922-11-08\n",
      "Identified events: 2602\n",
      "Identified books: 666\n",
      "Identified books without those read by member: 660\n",
      "Euclidean: 469, Cosine: 469, Pearson: 469\n",
      "Processing index: 1\n",
      "Subscription period: 1924-03-28 to 1925-03-28\n",
      "Identified events: 6112\n",
      "Identified books: 1292\n",
      "Identified books without those read by member: 1280\n",
      "Euclidean: 946, Cosine: 946, Pearson: 946\n"
     ]
    }
   ],
   "source": [
    "def get_final_predictions(partial_df: pd.DataFrame, events_df: pd.DataFrame, member_item_counts: pd.DataFrame, weighted: bool, n: int = 200, return_all: bool = True, limit_to_circulation: bool = False) -> List[pd.DataFrame]:\n",
    "\t\"\"\"\n",
    "\tThis function processes the partial_df DataFrame and returns a list of DataFrames with book recommendations.\n",
    "\n",
    "\tParameters:\n",
    "\tpartial_df (pd.DataFrame): The DataFrame to process.\n",
    "\tevents_df (pd.DataFrame): The DataFrame with event data.\n",
    "\tmember_item_counts (pd.DataFrame): The DataFrame with member item counts.\n",
    "\tweighted (bool): A flag indicating whether to use weighted predictions.\n",
    "\tn (int, optional): The number of top items to include in the DataFrame. Defaults to 200.\n",
    "\treturn_all (bool, optional): A flag indicating whether to return all items in the rankings. Defaults to True.\n",
    "\tlimit_to_circulation (bool, optional): A flag indicating whether to limit the books to those in circulation. Defaults to False.\n",
    "\n",
    "\tReturns:\n",
    "\tpd.DataFrame: A DataFrame with the top n items from the rankings.\n",
    "\t\"\"\"\n",
    "\tfinal_df = []\n",
    "\tprevious_books = None\n",
    "\tpartial_df = partial_df.reset_index(drop=True)\n",
    "\tfor index, row in partial_df.iterrows():\n",
    "\t\tprint(f\"Processing index: {index}\")\n",
    "\t\tprint(f\"Subscription period: {row.subscription_start} to {row.subscription_end}\")\n",
    "\n",
    "\t\tcirculation_events = events_df[(events_df.start_datetime < row.subscription_endtime) | (events_df.end_datetime < row.subscription_endtime)]\n",
    "\t\tprint(f\"Identified events: {len(circulation_events)}\")\n",
    "\n",
    "\t\tquery_books = circulation_events[circulation_events.item_uri.notna()].item_uri.unique().tolist()\n",
    "\t\tprint(f\"Identified books: {len(query_books)}\")\n",
    "\n",
    "\t\tmember_book_ids = events_df[(events_df.item_uri.notna()) & (events_df.member_id == row.member_id)].item_uri.unique()\n",
    "\t\tquery_books = list(set(query_books) - set(member_book_ids))\n",
    "\t\tprint(f\"Identified books without those read by member: {len(query_books)}\")\n",
    "\n",
    "\t\tif limit_to_circulation:\n",
    "\t\t\tif index == 0:\n",
    "\t\t\t\tprevious_books = query_books\n",
    "\t\t\telse:\n",
    "\t\t\t\tquery_books = list(set(query_books) - set(previous_books))\n",
    "\t\t\t\tprint(f\"Identified books without those read by member and those read in previous subscription: {len(query_books)}\")\n",
    "\n",
    "\t\trankings, cos_rankings, pear_rankings = get_predictions(row.member_id, member_item_counts, weighted, query_books)\n",
    "\t\tprint(f\"Euclidean: {len(rankings)}, Cosine: {len(cos_rankings)}, Pearson: {len(pear_rankings)}\")\n",
    "\n",
    "\t\teuc_df = create_rankings_dataframe(rankings, 'euclidean', n, return_all)\n",
    "\t\tcos_df = create_rankings_dataframe(cos_rankings, 'cosine', n, return_all)\n",
    "\t\tpear_df = create_rankings_dataframe(pear_rankings, 'pearson', n, return_all)\n",
    "\n",
    "\t\tdfs = pd.concat([euc_df, cos_df, pear_df])\n",
    "\t\tdfs['member_id'] = row.member_id\n",
    "\t\tdfs['period'] = row.subscription_start + '/' + row.subscription_end\n",
    "\t\tdfs['subscription_start'] = row.subscription_start\n",
    "\t\tdfs['subscription_end'] = row.subscription_end\n",
    "\t\tfinal_df.append(dfs)\n",
    "\t# Concatenate all the DataFrames in 'final_df' into a single DataFrame.\n",
    "\t# This creates a DataFrame with all the book recommendations.\n",
    "\tfinal_preds = pd.concat(final_df)\n",
    "\treturn final_preds\n",
    "\n",
    "print(\"Calculating predictions for all members, but limited to circulation periods...\")\n",
    "final_preds_all_limit_circulation = get_final_predictions(partial_df, events_df, member_item_counts, weighted, n=200, return_all=True, limit_to_circulation=True)\n",
    "print(\"Calculating predictions for all members...\")\n",
    "final_preds_all = get_final_predictions(partial_df, events_df, member_item_counts, weighted, n=200, return_all=True, limit_to_circulation=False)\n",
    "print(\"Calculating predictions for top 200 members, but limited to circulation periods...\")\n",
    "final_preds_top200_limit_circulation = get_final_predictions(partial_df, events_df, member_item_counts, weighted, n=200, return_all=False, limit_to_circulation=True)\n",
    "print(\"Calculating predictions for top 200 members...\")\n",
    "final_preds_top200 = get_final_predictions(partial_df, events_df, member_item_counts, weighted, n=200, return_all=False, limit_to_circulation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique recommended books: 946\n"
     ]
    }
   ],
   "source": [
    "def format_recommendations(final_preds: pd.DataFrame, return_all: bool = True) -> Tuple[List[str], pd.DataFrame]:\n",
    "\t\"\"\"\n",
    "\tThis function processes the final_preds DataFrame and returns a list of recommended books.\n",
    "\n",
    "\tParameters:\n",
    "\tfinal_preds (pd.DataFrame): The DataFrame to process.\n",
    "\treturn_all (bool, optional): A flag indicating whether to return all items in the rankings. Defaults to True.\n",
    "\n",
    "\tReturns:\n",
    "\tList[str]: A list of recommended books.\n",
    "\tpd.DataFrame: A DataFrame with the top n items from the rankings.\n",
    "\t\"\"\"\n",
    "\t# Pivot 'final_preds' to create a matrix where the rows are item URIs, formatted chart titles, formatted table titles, member IDs, periods, subscription start times, and subscription end times, the columns are score types, and the values are scores.\n",
    "\t# Fill missing values with 0.\n",
    "\t# This creates a user-item matrix that can be used for collaborative filtering.\n",
    "\tpivoted_predictions = pd.pivot(final_preds, index=['item_uri', 'formatted_chart_title', 'formatted_table_title', 'member_id', 'period', 'subscription_start', 'subscription_end'], columns='type', values='score').reset_index().fillna(0)\n",
    "\n",
    "\t# Scale the scores in 'final_preds' within each period and score type.\n",
    "\t# This can help improve the performance of the collaborative filtering algorithm.\n",
    "\tfinal_preds['score_scaled'] = final_preds.groupby(['period', 'type'])['score'].apply(lambda x: (x-min(x))/(max(x)-min(x)))\n",
    "\t# Define a list of metrics to scale.\n",
    "\tmetrics = ['cosine', 'pearson', 'euclidean']\n",
    "\n",
    "\t# Scale the metrics in 'pivoted_predictions'.\n",
    "\tscaled_predictions = scale_col(pivoted_predictions, metrics)\n",
    "\n",
    "\t# Melt 'scaled_predictions' to create a DataFrame where each row represents a book recommendation, with columns for the item URI, formatted chart title, formatted table title, member ID, period, subscription start time, subscription end time, metric, and score.\n",
    "\tpreds_df = pd.melt(scaled_predictions, id_vars=['item_uri', 'formatted_chart_title', 'formatted_table_title', 'member_id', 'period', 'subscription_start', 'subscription_end'], value_vars=['cosine_scaled', 'pearson_scaled', 'euclidean_scaled'], var_name='metric', value_name='score')\n",
    "\n",
    "\t# Replace the scaled metric names with the original metric names.\n",
    "\tpreds_df.loc[preds_df.metric == 'cosine_scaled', 'metric'] = 'cosine'\n",
    "\tpreds_df.loc[preds_df.metric == 'pearson_scaled', 'metric'] = 'pearson'\n",
    "\tpreds_df.loc[preds_df.metric == 'euclidean_scaled', 'metric'] = 'euclidean'\n",
    "\n",
    "\t# Sort 'preds_df' by score in descending order.\n",
    "\t# This orders the book recommendations by score, with the highest scores first.\n",
    "\tpreds_df = preds_df.sort_values(by=['score'] ,ascending=False)\n",
    "\n",
    "\t# Get the unique item URIs from the top 100 rows in 'preds_df'.\n",
    "\t# This gives us the top 100 recommended books.\n",
    "\titems = preds_df.item_uri.unique() if return_all else preds_df[0:100].item_uri.unique()\n",
    "\n",
    "\t# Print the number of unique recommended books.\n",
    "\tprint(f\"Number of unique recommended books: {len(items)}\")\n",
    "\n",
    "\treturn items, preds_df\n",
    "\n",
    "recommended_books, formatted_predictions = format_recommendations(final_preds, return_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique periods: 2\n"
     ]
    }
   ],
   "source": [
    "def subset_periods(formatted_predictions, return_all):\n",
    "\tif return_all:\n",
    "\t\tperiods = formatted_predictions.period.unique().tolist()\n",
    "\t\tprint(f\"Number of unique periods: {len(periods)}\")\n",
    "\t\treturn formatted_predictions\n",
    "\telse:\n",
    "\t\t# Get the unique periods from 'preds_df'.\n",
    "\t\tperiods = formatted_predictions.period.unique().tolist()\n",
    "\n",
    "\t\t# Set the number of top items to include for each period.\n",
    "\t\tn = 23\n",
    "\n",
    "\t\t# Initialize an empty list to store the final items for each period.\n",
    "\t\tfinal_items = []\n",
    "\n",
    "\t\t# For each period...\n",
    "\t\tfor index, period in enumerate(periods):\n",
    "\t\t\t# Initialize an empty list to store the items for this period.\n",
    "\t\t\titems = []\n",
    "\t\t\t\n",
    "\t\t\t# Get the rows from 'formatted_predictions' for this period that are not periodicals, and sort them by score in descending order.\n",
    "\t\t\trows = formatted_predictions[(formatted_predictions.period == period) & (formatted_predictions.formatted_chart_title.str.contains('Periodical') == False)].sort_values(by=['score'] ,ascending=False)\n",
    "\t\t\t\n",
    "\t\t\t# If this is the first period...\n",
    "\t\t\tif index == 0:\n",
    "\t\t\t\t# While the number of items is less than or equal to n...\n",
    "\t\t\t\twhile len(items) <= n:\n",
    "\t\t\t\t\t# Get the item URI of the first row.\n",
    "\t\t\t\t\titem = rows.item_uri.iloc[0]\n",
    "\t\t\t\t\t# If the item is not already in the list of items, add it.\n",
    "\t\t\t\t\tif item not in items:\n",
    "\t\t\t\t\t\titems.append(item)\n",
    "\t\t\t\t\t# Remove the first row from 'rows'.\n",
    "\t\t\t\t\trows = rows.drop(rows.index[0])\n",
    "\t\t\t\t# Add the list of items to 'final_items'.\n",
    "\t\t\t\tfinal_items.append(items)\n",
    "\t\t\t# If this is not the first period...\n",
    "\t\t\telse:\n",
    "\t\t\t\t# While the number of items is less than or equal to n...\n",
    "\t\t\t\twhile len(items) <= n:\n",
    "\t\t\t\t\t# Get the item URI of the first row.\n",
    "\t\t\t\t\titem = rows.item_uri.iloc[0]\n",
    "\t\t\t\t\t# If the item is not already in the list of items and it was not in the list of items for the previous period, add it.\n",
    "\t\t\t\t\tif (item not in items) and (item not in final_items[index-1]):\n",
    "\t\t\t\t\t\titems.append(item)\n",
    "\t\t\t\t\t# Remove the first row from 'rows'.\n",
    "\t\t\t\t\trows = rows.drop(rows.index[0])\n",
    "\t\t\t\t# Limit the list of items to the top n items.\n",
    "\t\t\t\titems = items[0:n]\n",
    "\t\t\t\t# Add the list of items to 'final_items'.\n",
    "\t\t\t\tfinal_items.append(items)\n",
    "\n",
    "\t\t# Print the number of items in the list for the second period.\n",
    "\t\tprint(f\"The number of items in the list for the second period is: {len(final_items[1])}\")\n",
    "\n",
    "\t\t# Concatenate the lists of top items for the first and second periods.\n",
    "\t\t# This gives us a list of all the top items across both periods.\n",
    "\t\ttop_items = final_items[0] + final_items[1] \n",
    "\n",
    "\t\t# Filter 'formatted_predictions' to include only rows where the item URI is in the list of top items.\n",
    "\t\t# Then, group the rows by period and count the number of unique item URIs for each period.\n",
    "\t\t# This gives us the number of unique recommended books for each period.\n",
    "\t\tunique_items_per_period = formatted_predictions[formatted_predictions.item_uri.isin(top_items)].groupby(['period'])['item_uri'].nunique()\n",
    "\t\tprint(unique_items_per_period)\n",
    "\n",
    "\t\ttop_results = formatted_predictions[formatted_predictions.item_uri.isin(top_items)]\n",
    "\t\ttop_results.to_csv('./public_data/memorycf_top_results.csv', index=False)\n",
    "\t\treturn top_results\n",
    "\t\n",
    "formatted_predictions = subset_periods(formatted_predictions, return_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group 'formatted_predictions' by member ID, subscription start and end times, period, item URI, and formatted table and chart titles.\n",
    "# Calculate the median, skewness, standard deviation, and variance of the scores for each group.\n",
    "# Reset the index of the resulting DataFrame and flatten the multi-level column names.\n",
    "metrics_df = formatted_predictions.groupby(['member_id', 'subscription_start', 'subscription_end', 'period', 'item_uri', 'formatted_table_title', 'formatted_chart_title']).agg({'score': [np.median, 'skew', 'std', 'var']}).reset_index()\n",
    "metrics_df.columns = list(map(''.join, metrics_df.columns.values))\n",
    "metrics_df.columns = [col if 'score' not in col else col.split('score')[1] for col in metrics_df.columns ]\n",
    "\n",
    "# Calculate the kurtosis of the scores for each group and add it to 'metrics_df'.\n",
    "kurt_df = formatted_predictions.groupby(['member_id', 'subscription_start', 'subscription_end', 'period', 'item_uri', 'formatted_table_title', 'formatted_chart_title'])['score'].apply(pd.DataFrame.kurt).reset_index(name='kurtosis')\n",
    "final_df = pd.merge(metrics_df, kurt_df, on=['member_id', 'subscription_start', 'subscription_end', 'period', 'item_uri', 'formatted_table_title', 'formatted_chart_title'])\n",
    "\n",
    "# Merge 'final_df' with 'formatted_predictions' to add the original scores.\n",
    "final_df = pd.merge(final_df, formatted_predictions, on=['member_id', 'subscription_start', 'subscription_end', 'period', 'item_uri', 'formatted_table_title', 'formatted_chart_title'], how='left')\n",
    "\n",
    "# Calculate the z-scores of the scores for each group.\n",
    "final_df['zscore'] = final_df.groupby(['member_id', 'subscription_start', 'subscription_end', 'period', 'item_uri', 'formatted_table_title', 'formatted_chart_title'])['score'].transform(lambda x : zscore(x,ddof=1))\n",
    "\n",
    "# Calculate the maximum score for each group and merge it with 'final_df'.\n",
    "top_scores = final_df.groupby(['member_id', 'subscription_start', 'subscription_end', 'period', 'item_uri', 'formatted_table_title', 'formatted_chart_title']).agg({'score':'max'})[['score']].reset_index()\n",
    "top_scores = pd.merge(top_scores, final_df, on=top_scores.columns.tolist(), how='inner')\n",
    "top_scores = top_scores.rename(columns={'score': 'top_score', 'zscore' : 'top_zscore'})\n",
    "\n",
    "# Calculate the average score for each group and merge it with 'top_scores'.\n",
    "avg_scores = final_df.groupby(['member_id', 'subscription_start', 'subscription_end', 'period', 'item_uri', 'formatted_table_title', 'formatted_chart_title'])['score'].mean().reset_index(name='avg_score')\n",
    "scores_df = pd.merge(top_scores, avg_scores, on=['member_id', 'subscription_start', 'subscription_end', 'period', 'item_uri', 'formatted_table_title', 'formatted_chart_title'])\n",
    "\n",
    "# Calculate the median score for each group and merge it with 'scores_df'.\n",
    "median_scores = final_df.groupby(['member_id', 'subscription_start', 'subscription_end', 'period', 'item_uri', 'formatted_table_title', 'formatted_chart_title'])['score'].median().reset_index(name='median_score')\n",
    "scores_df = pd.merge(scores_df, median_scores, on=['member_id', 'subscription_start', 'subscription_end', 'period', 'item_uri', 'formatted_table_title', 'formatted_chart_title'])\n",
    "\n",
    "# Calculate the standard deviation of the scores for each group and merge it with 'scores_df'.\n",
    "std_scores = final_df.groupby(['member_id', 'subscription_start', 'subscription_end', 'period', 'item_uri', 'formatted_table_title', 'formatted_chart_title']).agg({'score': 'std'}).reset_index()\n",
    "std_scores = std_scores.rename(columns={'score': 'std_score'})\n",
    "scores_df = pd.merge(scores_df, std_scores, on=['member_id', 'subscription_start', 'subscription_end', 'period', 'item_uri', 'formatted_table_title', 'formatted_chart_title'])\n",
    "\n",
    "# Calculate the mode of the scores for each group and merge it with 'final_df'.\n",
    "mode_scores = formatted_predictions.groupby(['member_id', 'period', 'subscription_start', 'subscription_end','item_uri'])['score'].agg(lambda x: pd.Series.mode(x).iat[0]).reset_index()\n",
    "mode_scores = pd.merge(mode_scores, final_df, on=mode_scores.columns.tolist(), how='inner')\n",
    "mode_scores = mode_scores.rename(columns={'score': 'mode_score', 'zscore' : 'mode_zscore'})\n",
    "\n",
    "# Merge 'mode_scores' with 'scores_df' to create the final DataFrame of scores.\n",
    "final_scores = pd.merge(mode_scores[['member_id', 'period', 'item_uri', 'formatted_table_title', 'formatted_chart_title', 'mode_score', 'mode_zscore', 'subscription_start', 'subscription_end']], scores_df, on=['member_id', 'period', 'item_uri', 'formatted_table_title', 'formatted_chart_title', 'subscription_start', 'subscription_end'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deduping final scores: (1740, 20) -> (947, 21)\n"
     ]
    }
   ],
   "source": [
    "# Select the columns for member ID, period, item URI, formatted table and chart titles, and subscription start and end times from 'final_scores'.\n",
    "# Drop duplicate rows to get a DataFrame of unique member subscriptions.\n",
    "member_subscriptions = final_scores[['member_id', 'period', 'item_uri', 'formatted_table_title', 'formatted_chart_title', 'subscription_start', 'subscription_end']].drop_duplicates()\n",
    "\n",
    "# Drop duplicate rows from 'final_scores' to get a DataFrame of unique scores.\n",
    "final_scores_dedup = final_scores.drop_duplicates()\n",
    "\n",
    "# Calculate the coefficient of variation (standard deviation divided by median) for each score in 'final_scores_dedup'.\n",
    "# Add the coefficient of variation to 'final_scores_dedup' as a new column.\n",
    "final_scores_dedup['coef_var'] = (final_scores_dedup.std_score/ final_scores_dedup.median_score)\n",
    "\n",
    "print(f\"Deduping final scores: {final_scores.shape} -> {final_scores_dedup.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_predictions.to_csv('./data/full_scores_collaborative_filterting_without_periodicals_circulation_limited.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_scores_dedup.to_csv('./data/aggregated_full_scores_collaborative_filtering_without_periodicals_circulation_limited.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0fb780361d6b371d92e35df6f1912f5955cee6289dee7ee73aaeea669179a72d"
  },
  "kernelspec": {
   "display_name": "unknown_borrowers_updated_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
